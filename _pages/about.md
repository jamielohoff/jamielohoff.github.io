---
title: About Me
permalink: /about/
layout: single
author_profile: true
---

# TLDR
**Upbringing and age**: I was born on March 11th, 1996 in Illertissen, Bavaria and grew up in the suburbs of Stuttgart (you do the math yourself).<br>
**Marital status**: Married to the most beautiful woman in the world (10/10, would do again).<br>
**Education**: Studied Physics in Stuttgart and Heidelberg (BS, MS).<br>
**Occupation**: Doing a PhD in Neuromorphic Computing and Artificial Intelligence at RWTH Aachen and JÃ¼lich Research Center. It's a lot of fun!<br>


# The Scientific Me
I am yet another physicist who left the field because auf the Neuroscience and AI hype. 
This however does not mean that I have given up on all the physics and math; quite the contrary actually.
What intrigues me about Neuroscience and AI is the lack of good foundational theory that allows us to truly understand what makes animals, humans and machines intelligent.
Is intelligence an emergent property that results from scaling up? Is it the connectome of the network that matters? Or is it the computational substrate itself?<br>
My guess would be that it is a superposition of all these effects, but we don't know yet how to combine these ingredients in a systematic way.
I believe that insights from mathematics, physics, control theory and electrical engineering can be of help there.
I am particularly interested in applying methods from Statistical Physics and Differential Geometry to these problems an see how far I can get with that.
Why? No specific reason. I just like the ideas and the math and hope to have fun along the way.
If I had to justify it though, I would probably give an answer similar to:<br>

> Statistical physics is the study of large-scale systems composed of microscopic entities to understand their macroscopic, collective behavior.
It is thus a good choice to study whether intelligence is an emergent property of ensembles of neurons.

<br><br>

>Differential Geometry enables one to study optimization. In my view learning and optimization are very similar; sometimes event the same, e.g. supervised learning.
Therefore it is a good framework to think about learning. Maybe.<br>

Apart from these very theoretical considerations, I am also a big fan of actually building cool and useful things, which is also why I want to understand Neuroscience and AI better in the first place. I am convinced that we can only build cooler and better stuff if we really understand what we are doing.
<br><br>

Neuroscience-inspired stuff that I had the pleasure to work with:
- Event-based cameras that work like the human retina
- Spiking neural network accelerators aka. silicon brains (BrainScalesS, Speck, DynapCNN)
- BPTT, RTRL, E-Prop, Event-Prop and all the other famous learning rules

AI stuff that I have enjoyed working with so far:
- Transformers (who hasn't though...)
- Reinforcement Learning, in particular PPO and AlphaZero
- Neural Differential Equations, although it is just the Pontryagin Principle from Control Theory at work
- All the default AI crap like CNNs, RNNs, LSTMs, MLPs, ResNets

Physics stuff that I came across which is not in the default curriculum:
- Cosmology, Quintessence models, Brans-Dicke, Sting Theory
- Infrared telescopy, especially the trajectory of the star S2 orbiting the Milkyway Central Blackhole
- Quantum Field Theory, although the Renormalization of QED is still kind of a fucked-up thing to me, Symmetry-breaking and Goldstone Bosons are a cool theory
- Topological Edge States, Topological Insulators, Topological QFT, but only the basics
- Differential Geometry and Topology (my favorite field, only non-standard for a physicist)

CV: <br>

# The more interesting Me
**Hobbies**:
- Hiking, Bouldering and Rock Climbing
- Everying that goes fast and is dangerous, e.g. Cars, Motorcycles, Skiing, Mountain Biking
- Woodworking (see Other Projects)
- Guitar and music, especially Rock (Guns'n'Roses), Jazz (Herbie Hancock, Oscar Petterson) and Classical Music (Beethoven)




